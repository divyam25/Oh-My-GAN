{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN-PyT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Oohb8pmyeNOO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "a45eb431-0dcf-4a8d-f59d-3636459bf229"
      },
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/60/66415660aa46b23b5e1b72bc762e816736ce8d7260213e22365af51e8f9c/torch-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (591.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 591.8MB 31kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x60a6e000 @  0x7f82414532a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "\u001b[?25hCollecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 21.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 3.9MB/s \n",
            "\u001b[?25hInstalling collected packages: torch, pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.3.0 torch-1.0.0 torchvision-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xQLnHK9jJwrU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os,time,itertools,pickle,imageio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i0Gh0dPiigMh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch as t\n",
        "from torch import cuda,nn,optim,utils\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets,transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P8YV4_-7xNsh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#DCGAN Model (Radford et.al 2015)"
      ]
    },
    {
      "metadata": {
        "id": "lVw3E5WUjuSh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#ThePolice\n",
        "\n",
        "class discriminator(nn.Module):\n",
        "    # initializers\n",
        "    def __init__(self, d):\n",
        "        super(discriminator, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, d, 4, 2, 1)\n",
        "        self.conv2 = nn.Conv2d(d, d*2, 4, 2, 1)\n",
        "        self.conv2_bn = nn.BatchNorm2d(d*2)\n",
        "        self.conv3 = nn.Conv2d(d*2, d*4, 4, 2, 1)\n",
        "        self.conv3_bn = nn.BatchNorm2d(d*4)\n",
        "        self.conv4 = nn.Conv2d(d*4, d*8, 4, 2, 1)\n",
        "        self.conv4_bn = nn.BatchNorm2d(d*8)\n",
        "        self.conv5 = nn.Conv2d(d*8, 1, 4, 1, 0)\n",
        "\n",
        "    # weight_init... for stable gradient flow across layers\n",
        "    def weight_init(self, mean, std):\n",
        "        for m in self._modules:\n",
        "            normal_init(self._modules[m], mean, std)\n",
        "    \n",
        "    # forward method\n",
        "    def forward(self, input):\n",
        "        x = F.leaky_relu(self.conv1(input), 0.2)             #LeakyReLU in all layers except last\n",
        "        x = F.leaky_relu(self.conv2_bn(self.conv2(x)), 0.2)\n",
        "        x = F.leaky_relu(self.conv3_bn(self.conv3(x)), 0.2)\n",
        "        x = F.leaky_relu(self.conv4_bn(self.conv4(x)), 0.2)\n",
        "        x = F.sigmoid(self.conv5(x))\n",
        "\n",
        "        return x\n",
        "      \n",
        "#TheCulprit\n",
        "\n",
        "class generator(nn.Module):\n",
        "    # initializers\n",
        "    def __init__(self, d):\n",
        "        super(generator, self).__init__()\n",
        "        self.deconv1 = nn.ConvTranspose2d(100, d*8, 4, 1, 0)     #100-d latent space vector\n",
        "        self.deconv1_bn = nn.BatchNorm2d(d*8)\n",
        "        self.deconv2 = nn.ConvTranspose2d(d*8, d*4, 4, 2, 1)\n",
        "        self.deconv2_bn = nn.BatchNorm2d(d*4)\n",
        "        self.deconv3 = nn.ConvTranspose2d(d*4, d*2, 4, 2, 1)\n",
        "        self.deconv3_bn = nn.BatchNorm2d(d*2)\n",
        "        self.deconv4 = nn.ConvTranspose2d(d*2, d, 4, 2, 1)\n",
        "        self.deconv4_bn = nn.BatchNorm2d(d)\n",
        "        self.deconv5 = nn.ConvTranspose2d(d, 1, 4, 2, 1)\n",
        "\n",
        "    # weight_init\n",
        "    def weight_init(self, mean, std):\n",
        "        for m in self._modules:\n",
        "            normal_init(self._modules[m], mean, std)\n",
        "\n",
        "    # forward method\n",
        "    def forward(self, input):\n",
        "        # x = F.relu(self.deconv1(input))\n",
        "        x = F.relu(self.deconv1_bn(self.deconv1(input)))  #ReLU all layers except last where tanh used\n",
        "        x = F.relu(self.deconv2_bn(self.deconv2(x)))\n",
        "        x = F.relu(self.deconv3_bn(self.deconv3(x)))\n",
        "        x = F.relu(self.deconv4_bn(self.deconv4(x)))\n",
        "        x = F.tanh(self.deconv5(x))\n",
        "\n",
        "        return x\n",
        "      \n",
        "#WeightInitialiser\n",
        "def normal_init(m, mean, std):\n",
        "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
        "        m.weight.data.normal_(mean, std)\n",
        "        m.bias.data.zero_()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hJ3GA2UvyQPn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Utilities"
      ]
    },
    {
      "metadata": {
        "id": "wHUP1sMEyV3m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#As mentioned in the paper\n",
        "param = {\n",
        "    \"batch_sz\" : 128,\n",
        "    \"lr\" : 0.0002,\n",
        "    \"beta\" : 0.5,\n",
        "    \"img_size\" : 64}\n",
        "\n",
        "tr_epch = 15\n",
        "\n",
        "#data transforms\n",
        "transform = transforms.Compose([\n",
        "        transforms.Resize(param[\"img_size\"]),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "#Load MNIST\n",
        "train_loader = utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=True, download=True, transform=transform),\n",
        "    batch_size=param[\"batch_sz\"], shuffle=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hM5ZvC9H8oYO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Model_init\n",
        "g = generator(128)\n",
        "d = discriminator(128)\n",
        "g.weight_init(mean=0.0, std=0.02)\n",
        "d.weight_init(mean=0.0, std=0.02)\n",
        "\n",
        "#cost function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "if cuda.is_available() :\n",
        "  g.cuda() \n",
        "  d.cuda()\n",
        "  criterion.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CnJJmAGy_bux",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "g_optim = optim.Adam(g.parameters(), lr=param[\"lr\"], betas=(param[\"beta\"], 0.999))\n",
        "d_optim = optim.Adam(d.parameters(), lr=param[\"lr\"], betas=(param[\"beta\"], 0.999))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G05RHOVmJEaW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Train Iterator"
      ]
    },
    {
      "metadata": {
        "id": "N5GsBos6dcSn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#DejaVu\n",
        "hist = {}\n",
        "hist['D_loss'] = []\n",
        "hist['G_loss'] = []\n",
        "hist['per_epoch_time'] = []\n",
        "hist['tot_time'] = []\n",
        "num_iter = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_DoDYeSaGg5P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Training Starts Now\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(tr_epch) :\n",
        "  \n",
        "  epoch_start_time = time.time()\n",
        "  d_loss = []\n",
        "  g_loss = []\n",
        "  \n",
        "  for i,data in enumerate(train_loader) : \n",
        "  \n",
        "    y_real = t.ones(param[\"batch_sz\"])   #Alternatively t.ones(data[0].size(0))\n",
        "    y_fake = t.zeros(param[\"batch_sz\"])\n",
        "    x_real = data[0]\n",
        "    z_d = t.randn((param[\"batch_sz\"], 100)).view(-1, 100, 1, 1) #Uniform distribution\n",
        "    z_g = t.randn((param[\"batch_sz\"], 100)).view(-1, 100, 1, 1) #Uniform distribution\n",
        "    \n",
        "    if cuda.is_available() :\n",
        "      #Variable is autograd compliant as of 0.4.1\n",
        "      x_real,y_real,y_fake,z_d,z_g = Variable(x_real.cuda()),Variable(y_real.cuda()),Variable(y_fake.cuda()),Variable(z_d.cuda()),Variable(z_g.cuda())\n",
        "    \n",
        "    #Police Training\n",
        "    \n",
        "    #Discriminator Loss A (Classifies real samples as real....pretty intuitive)\n",
        "    D_result = d(x_real).squeeze()\n",
        "    D_loss_A = criterion(D_result,y_real)\n",
        "    \n",
        "    #Discriminator loss B (classifies fake samples as fake)\n",
        "    D_result = d(g(z_d)).squeeze()\n",
        "    D_loss_B = criterion(D_result,y_fake)\n",
        "    \n",
        "    #Total Discriminator Loss = Loss A + Loss B\n",
        "    D_loss_total = D_loss_A + D_loss_B\n",
        "    d_loss.append(D_loss_total.data[0])   #Future Use\n",
        "    \n",
        "    d.zero_grad()\n",
        "    D_loss_total.backward()\n",
        "    d_optim.step()\n",
        "    \n",
        "    #Culprit Training\n",
        "    \n",
        "    #Generator Loss (generates real looking fake samples)\n",
        "    G_result = d(g(z_g)).squeeze()\n",
        "    G_loss_total = criterion(G_result,y_real)\n",
        "    g_loss.append(G_loss_total.data[0])\n",
        "    \n",
        "    g.zero_grad()\n",
        "    G_loss_total.backward()\n",
        "    g_optim.step()\n",
        "    \n",
        "    num_iter += 1\n",
        "  \n",
        "  epoch_end_time = time.time()\n",
        "  per_epoch_time = epoch_end_time - epoch_start_time\n",
        "  print('[%d/%d] - ptime:%.2f, loss_d: %.3f, loss_g: %.3f' % ((epoch + 1), tr_epch, per_epoch_time, torch.mean(torch.FloatTensor(d_loss)),\n",
        "                                                              torch.mean(torch.FloatTensor(g_loss))))\n",
        "  hist['D_loss'].append(torch.mean(torch.FloatTensor(d_loss)))\n",
        "  hist['G_loss'].append(torch.mean(torch.FloatTensor(g_loss)))\n",
        "  hist['per_epoch_times'].append(per_epoch_time)\n",
        "  \n",
        "  \n",
        "  \n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "hist['tot_time'].append(total_time)\n",
        "print(\"Avg per epoch time: %.2f, total %d epochs time: %.2f\" % (torch.mean(torch.FloatTensor(hist['per_epoch_ptimes'])), tr_epch, total_time))\n",
        "print(\"Training over!..\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}